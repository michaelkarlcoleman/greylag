#!/bin/bash
#        $Id$

# This directory is assumed to be shared across all cluster nodes
SHAREDTMPDIR=/clusterfs/home/proteomics/tmp

# The python image to use
PYTHON=/n/site/inst/Linux-i686/sys/bin/python2.4

# greylag lives here (use python2.4 explicitly)
greylag=/clusterfs/site/inst/Linux-i686/bioinfo/greylag/greylag.py

# options passed to all greylag invocations
#greylag_options="-v --quirks-mode"
greylag_options="-v"

# Divide this run into this many parts.  This should probably be at least 5x
# the number of cluster nodes available.  Currently, the fixed overhead cost
# for each part is pretty small (10-20 seconds?).
PARTS=5000


p=`basename $0`

usage() {
    cat <<EOF 1>&2
usage: $p [-l] <greylag-params.xml> [ <ms2-file> ... ]

Process a set of ms2 files as specified by the given params file (which must
end in '.xml' and should probably be in the current directory).

If the '-l' flag is given, stdout and stderr will be redirected to a
corresponding '.log' file.

EOF
    exit 0
}

err() {
    echo 1>&2 "$p: $@"
}

die() {
    err "$@"
    touch "$jobname.done-failed" || err "touch failed"
    exit 1
}

if [ "$1" == "-l" ]; then
    shift 
    logging=1
fi

if [ $# -lt 2 ]; then
    usage
fi

params="$1"

rm -f ${params%.xml}.done-*

case "$params" in
    *.xml) true;;
    *) die "parameter filename must end in '*.xml'";;
esac

jobname=$(basename $params .xml)

shift
for f in "$@"; do
    if [ $(dirname $f) != "." ]; then
	die "file '$f' must be in the current working directory"
    fi
    case "$f" in
	*.ms2 | *.ms2.gz | *.ms2.bz2) true;;
	*) die "argument file '$f' should end in '.ms2' (or '.ms2.{gz,bz2}')";;
    esac
done

[ -e $params ] || die "'$params' not found"

if ! ls -ld . | egrep -q '^drwxrws'; then
    chmod g+rwxs . || err "attempt to make this directory group writable failed"
fi


# Do some basic locking.  This tries to prevent simultaneous runs on the same
# parameter file, which would produce output to the same file, wasting
# resources and causing confusion.

lockfile="$PWD/$jobname.lock"
trap "rm -f $lockfile" EXIT
ln -s $$ $lockfile 2>/dev/null || true
lockpid=$(ls -ld $lockfile | sed -e 's/^.*> //')

if [ $lockpid != $$ ]; then
    die "this directory locked by another process (pid = $lockpid)?
         remove $lockfile if not"
fi

# point of no return
if [ "$logging" == "1" ]; then
    exec < /dev/null > "$jobname.log" 2>&1
fi

# priority?

# Be very careful with quoting, as these names may eventually come from
# Windows users...

shared_d=$SHAREDTMPDIR/greylag-$(date +%s)-$$ # unique

# Could add removal of the shared directory to the EXIT trap, but probably we
# shouldn't because pdq (and maybe SGE) may react badly
#    trap "rm -rf $lockfile $shared_d &" EXIT

mkdir $shared_d || die "'mkdir $shared_d' failed!"

cp -p "$params" $shared_d/ || die "params cp failed!"

for f in "$@"; do
    case "$f" in
	*.gz) zcat < "$f" > "$shared_d/$f" || die "ms2 zcat failed";;
	*.bz2) bzcat < "$f" > "$shared_d/$f" || die "ms2 zcat failed";;
	*) ln -s "$PWD/$f" $shared_d/ || die "ms2 ln -s failed";;
    esac
done

results="$jobname.out.xml.gz"

jobbasedir=$(basename $PWD)

#########################################################################
# submit the job in this shared directory so that the nodes can see it  
pushd $shared_d > /dev/null || die "pushd failed!"

# use err

#cat <<EOF
#job name: $jobname
#shared dir: $shared_d
#params: $params
#ms2: $@
#EOF
#ls -l $shared_d/

err round 0 setup

# Use of *.ms2 implies that there are no such files here except for the ones
# we just created.  Fix?
# [NB: the ms2 arguments must be given in the same order each time!]

# split ms2 files (on master, for now)
$PYTHON $greylag $greylag_options \
    -o "$results" \
    --part-split=$PARTS \
    "$params" *.ms2 \
    || die "round 0 setup on master failed"

err round 0 processing

# The -pe option is currently a hack to try to stop multiple jobs from being
# run on the same node.

# first do parts on nodes
qsub -sync y -r y -b y -cwd -V -hard -l mem_free=250M \
    -q all.q \
    -t 1-$PARTS \
    -N "greylag-$jobbasedir-$jobname-$$" \
    -e 'round-1.$TASK_ID.err' \
    -o 'round-1.$TASK_ID.out' \
    $PYTHON $greylag $greylag_options \
    -o "$results" \
    --part='${SGE_TASK_ID}of${SGE_TASK_LAST}' \
    "$params" *.ms2
qsub_status=$?
cat $(ls -1 round-1.*.{out,err} | sort -t . -k 2n,3) /dev/null
if [ "$qsub_status" != 0 ]; then
    die "round 0 on nodes failed"
fi

err round 0 merge

# now merge results (on master, for now)
$PYTHON $greylag $greylag_options \
    -o "$results" \
    --part-merge=$PARTS \
    "$params" *.ms2 \
    || die "round 0 merge on master failed"

popd > /dev/null || die "popd failed!"
#########################################################################

if ! [ -e $shared_d/"$results" ]; then
    die 'no results file present after processing--job failed?'
fi

cp -p $shared_d/"$results" . || die "cp failed!"

# for now, don't do this
#rm -fr "$shared_d" $lockfile

err complete
touch "$jobname.done-ok" || err "touch failed"
exit 0
